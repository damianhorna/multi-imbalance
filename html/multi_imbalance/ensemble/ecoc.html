<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>multi_imbalance.ensemble.ecoc API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>multi_imbalance.ensemble.ecoc</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os

import numpy as np
from imblearn.over_sampling import SMOTE
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.utils import check_random_state
from sklearn.model_selection import train_test_split
from collections import Counter
from collections import defaultdict
from multi_imbalance.resampling.global_cs import GlobalCS
from multi_imbalance.resampling.soup import SOUP


class ECOC:
    &#34;&#34;&#34;
    ECOC (Error Correcting Output Codes) is ensemble method for multi-class classification problems.
    Each class is encoded with unique binary or ternary code (where 0 means that class is excluded from training set
    of binary classifier). Then in the learning phase each binary classifier is learned. In the decoding phase the class
    which is closest to test instance in the sense of Hamming distance is chosen.
    &#34;&#34;&#34;

    _allowed_encodings = [&#39;dense&#39;, &#39;sparse&#39;, &#39;complete&#39;, &#39;OVA&#39;, &#39;OVO&#39;]
    _allowed_oversampling = [None, &#39;globalCS&#39;, &#39;SMOTE&#39;, &#39;SOUP&#39;]
    _allowed_classifiers = [&#39;tree&#39;, &#39;NB&#39;, &#39;KNN&#39;]
    _allowed_weights = [None, &#39;acc&#39;, &#39;avg_tpr_min&#39;]

    def __init__(self, binary_classifier=&#39;KNN&#39;, preprocessing=&#39;SOUP&#39;, encoding=&#39;OVO&#39;, n_neighbors=3,
                 weights=None):
        &#34;&#34;&#34;
        Parameters
        ----------
        binary_classifier: binary classifier used by the algorithm. Possible classifiers:
        * &#39;tree&#39;: Decision Tree Classifier,
        * &#39;NB&#39;: Naive Bayes Classifier,
        * &#39;KNN&#39; : K-Nearest Neighbors

        preprocessing: method for oversampling between aggregated classes in each dichotomy. Possible methods:
        * None : no oversampling applied,
        * &#39;globalCS&#39; : random oversampling - randomly chosen instances of minority classes are duplicated
        * &#39;SMOTE&#39; : Synthetic Minority Oversampling Technique
        * &#39;SOUP&#39; : Similarity Oversampling Undersampling Preprocessing

        encoding : algorithm for encoding classes. Possible encodings:
        * &#39;dense&#39;: ceil(10log2(num_of_classes)) dichotomies, -1 and 1 with probability 0.5 each
        * &#39;sparse&#39; : ceil(10log2(num_of_classes)) dichotomies, 0 with probability 0.5, -1 and 1 with probability 0.25 each
        * &#39;OVO&#39; : &#39;one vs one&#39; - n(n-1)/2 dichotomies, where n is number of classes, one for each pair of classes.
         Each column has one 1 and one -1 for classes included in particular pair, 0s for remaining classes.
        * &#39;OVA&#39; : &#39;one vs all&#39; - number of dichotomies is equal to number of classes. Each column has one 1 and
            -1 for all remaining rows
        * &#39;complete&#39; : 2^(n-1)-1 dichotomies, reference
            T. G. Dietterich and G. Bakiri.
            Solving multiclass learning problems via error-correcting output codes.
            Journal of Artificial Intelligence Research, 2:263–286, 1995.

        weights: strategy for dichotomies weighting. Possible values:
        * None : no weighting applied
        * &#39;acc&#39; : accuracy-based weights
        * &#39;avg_tpr_min&#39; : weights based on average true positive rates of dichotomies

        &#34;&#34;&#34;
        self.binary_classifier = binary_classifier
        self.encoding = encoding
        self.preprocessing = preprocessing
        self.n_neighbors = n_neighbors
        self.weights = weights

        self.minority_classes = list()

        self._code_matrix = None
        self._binary_classifiers = []
        self._labels = None
        self._dich_weights = None

    def fit(self, X, y, minority_classes=None):
        &#34;&#34;&#34;
        Parameters
        ----------
        X: two dimensional numpy array (number of samples x number of features) with float numbers
        y: one dimensional numpy array with labels for rows in X
        minority_classes: list of classes considered to be minority classes
        Returns
        -------
        self: object
        &#34;&#34;&#34;
        if minority_classes is not None:
            self.minority_classes = minority_classes

        if self.weights is not None:
            X_train, X_for_weights, y_train, y_for_weights = train_test_split(X, y, test_size=0.2, stratify=y,
                                                                              random_state=0)
        else:
            X_train, y_train = X, y

        self._labels = np.unique(y)
        self._gen_code_matrix()
        self._binary_classifiers = [self._get_classifier() for _ in range(self._code_matrix.shape[1])]
        self._learn_binary_classifiers(X_train, y_train)
        if self.weights is not None:
            self._calc_weights(X_for_weights, y_for_weights)
        return self

    def predict(self, X):
        &#34;&#34;&#34;
        Parameters
        ----------
        X: two dimensional numpy array (number of samples x number of features) with float numbers
        Returns
        -------
        y : numpy array, shape = [number of samples]
            Predicted target values for X.
        &#34;&#34;&#34;
        output_codes = np.zeros((X.shape[0], self._code_matrix.shape[1]))
        for classifier_idx, classifier in enumerate(self._binary_classifiers):
            output_codes[:, classifier_idx] = classifier.predict(X)

        predicted = np.zeros(X.shape[0])
        for row_idx, encoded_row in enumerate(output_codes):
            predicted[row_idx] = self._get_closest_class(encoded_row)

        return predicted

    def _learn_binary_classifiers(self, X, y):
        for classifier_idx, classifier in enumerate(self._binary_classifiers):
            excluded_classes_indices = [idx for idx in range(len(y)) if
                                        self._code_matrix[self._labels.tolist().index(y[idx])][classifier_idx] == 0]
            X_filtered = np.delete(X, excluded_classes_indices, 0)
            y_filtered = np.delete(y, excluded_classes_indices)
            binary_labels = np.array([self._code_matrix[self._labels.tolist().index(clazz)][classifier_idx] for clazz in
                                      y_filtered])
            X_filtered, binary_labels = self._oversample(X_filtered, binary_labels)
            classifier.fit(X_filtered, binary_labels)

    def _gen_code_matrix(self):
        if self.encoding == &#39;dense&#39;:
            self._code_matrix = self._encode_dense(self._labels.shape[0])
        elif self.encoding == &#39;sparse&#39;:
            self._code_matrix = self._encode_sparse(self._labels.shape[0])
        elif self.encoding == &#39;complete&#39;:
            self._code_matrix = self._encode_complete(self._labels.shape[0])
        elif self.encoding == &#39;OVO&#39;:
            self._code_matrix = self._encode_ovo(self._labels.shape[0])
        elif self.encoding == &#39;OVA&#39;:
            self._code_matrix = self._encode_ova(self._labels.shape[0])
        else:
            raise ValueError(&#34;Unknown matrix generation encoding: %s, expected to be one of %s.&#34;
                             % (self.encoding, ECOC._allowed_encodings))

    def _encode_dense(self, number_of_classes, random_state=0, number_of_code_generations=10000):
        try:
            dirname = os.path.dirname(__file__)
            matrix = np.load(dirname + f&#39;/cached_matrices/dense_{number_of_classes}.npy&#39;)
            return matrix
        except IOError:
            print(f&#39;Could not find cached matrix for dense code for {number_of_classes} classes, generating matrix...&#39;)

        number_of_columns = int(np.ceil(10 * np.log2(number_of_classes)))
        code_matrix = np.ones((number_of_classes, number_of_columns))
        random_state = check_random_state(random_state)

        max_min_dist = 0
        for i in range(number_of_code_generations):
            tmp_code_matrix = np.ones((number_of_classes, number_of_columns))
            min_dist = float(&#39;inf&#39;)

            for row in range(0, number_of_classes):
                for col in range(0, number_of_columns):
                    if random_state.randint(0, 2) == 1:
                        tmp_code_matrix[row, col] = -1

                for compared_row in range(0, row):
                    dist = self._hamming_distance(tmp_code_matrix[compared_row], tmp_code_matrix[row])
                    if dist &lt; min_dist:
                        min_dist = dist

            if min_dist &gt; max_min_dist:
                max_min_dist = min_dist
                code_matrix = tmp_code_matrix
        return code_matrix

    def _encode_sparse(self, number_of_classes, random_state=0, number_of_code_generations=10000):
        try:
            dirname = os.path.dirname(__file__)
            matrix = np.load(dirname + f&#39;/cached_matrices/sparse_{number_of_classes}.npy&#39;)
            return matrix
        except IOError:
            print(f&#39;Could not find cached matrix for sparse code for {number_of_classes} classes, generating matrix...&#39;)

        number_of_columns = int(np.ceil(15 * np.log2(number_of_classes)))
        code_matrix = np.ones((number_of_classes, number_of_columns))
        random_state = check_random_state(random_state)

        max_min_dist = 0
        for i in range(number_of_code_generations):
            tmp_code_matrix = np.ones((number_of_classes, number_of_columns))
            min_dist = float(&#39;inf&#39;)

            for row in range(0, number_of_classes):
                for col in range(0, number_of_columns):
                    rand = random_state.randint(0, 4)
                    if rand &lt; 2:
                        tmp_code_matrix[row, col] = 0
                    elif rand == 3:
                        tmp_code_matrix[row, col] = -1

                if np.count_nonzero(tmp_code_matrix[row]) == 0:
                    break

                for compared_row in range(0, row):
                    dist = self._hamming_distance(tmp_code_matrix[compared_row], tmp_code_matrix[row])
                    if dist &lt; min_dist:
                        min_dist = dist

            if self._has_matrix_all_zeros_column(tmp_code_matrix):
                continue

            if min_dist &gt; max_min_dist:
                max_min_dist = min_dist
                code_matrix = tmp_code_matrix

        return code_matrix

    def _encode_ova(self, number_of_classes):
        matrix = np.identity(number_of_classes)
        matrix[matrix == 0] = -1
        return matrix

    def _encode_ovo(self, number_of_classes):
        number_of_columns = int(number_of_classes * (number_of_classes - 1) / 2)
        matrix = np.zeros((number_of_classes, number_of_columns), dtype=int)
        indices_map = self._map_indices_to_class_pairs(number_of_classes)
        for row in range(number_of_classes):
            for col in range(number_of_columns):
                if indices_map[col][0] == row:
                    matrix[row, col] = 1
                elif indices_map[col][1] == row:
                    matrix[row, col] = -1
        return matrix

    def _map_indices_to_class_pairs(self, number_of_classes):
        indices_map = dict()
        idx = 0
        for i in range(number_of_classes):
            for j in range(i + 1, number_of_classes):
                indices_map[idx] = (i, j)
                idx += 1
        return indices_map

    def _encode_complete(self, number_of_classes):
        code_length = 2 ** (number_of_classes - 1) - 1
        matrix = np.ones((number_of_classes, code_length))
        for row_idx in range(1, number_of_classes):
            digit = -1
            partial_code_len = 2 ** (number_of_classes - row_idx - 1)
            for idx in range(0, code_length, partial_code_len):
                matrix[row_idx][idx:idx + partial_code_len] = digit
                digit *= -1
        return matrix

    def _hamming_distance(self, v1, v2):
        return np.count_nonzero(v1 != v2)

    def _has_matrix_all_zeros_column(self, matrix):
        return (~matrix.any(axis=0)).any()

    def _get_closest_class(self, row):
        if self.weights is not None:
            return self._labels[
                np.argmin(
                    [sum(np.multiply(self.dich_weights, (encoded_class - row) ** 2)) for encoded_class in
                     self._code_matrix])]
        else:
            return self._labels[
                np.argmin([self._hamming_distance(row, encoded_class) for encoded_class in self._code_matrix])]

    def _oversample(self, X, y):
        if self.preprocessing not in ECOC._allowed_oversampling:
            raise ValueError(&#34;Unknown matrix generation encoding: %s, expected to be one of %s.&#34;
                             % (self.encoding, ECOC._allowed_oversampling))
        elif np.unique(y).size == 1:
            return X, y
        elif self.preprocessing is None:
            return X, y
        elif self.preprocessing == &#39;globalCS&#39;:
            gcs = GlobalCS()
            return gcs.fit_transform(X, y)
        elif self.preprocessing == &#39;SMOTE&#39;:
            return self._smote_oversample(X, y)
        elif self.preprocessing == &#39;SOUP&#39;:
            soup = SOUP()
            return soup.fit_transform(X, y)

    def _get_classifier(self):
        if self.binary_classifier not in ECOC._allowed_classifiers:
            raise ValueError(&#34;Unknown binary classifier: %s, expected to be one of %s.&#34;
                             % (self.binary_classifier, ECOC._allowed_classifiers))
        elif self.binary_classifier == &#39;tree&#39;:
            decision_tree_classifier = DecisionTreeClassifier(random_state=42)
            return decision_tree_classifier
        elif self.binary_classifier == &#39;NB&#39;:
            gnb = GaussianNB()
            return gnb
        elif self.binary_classifier == &#39;KNN&#39;:
            knn = KNeighborsClassifier(n_neighbors=self.n_neighbors)
            return knn

    def _smote_oversample(self, X, y):
        n_neighbors = min(3, min(np.unique(y, return_counts=True)[1]) - 1)
        if n_neighbors == 0:
            raise ValueError(
                &#39;In order to use SMOTE preprocessing, the training set should contain at least 2 examples from each class&#39;)
        smote = SMOTE(k_neighbors=n_neighbors, random_state=42)
        return smote.fit_resample(X, y)


    def _calc_weights(self, X_for_weights, y_for_weights):
        if self.weights not in ECOC._allowed_weights:
            raise ValueError(&#34;Unknown weighting strategy: %s, expected to be one of %s.&#34;
                             % (self.weights, ECOC._allowed_weights))

        dich_weights = np.ones(self._code_matrix.shape[1])
        if self.weights == &#39;acc&#39;:
            for clf_idx, clf in enumerate(self._binary_classifiers):
                samples_no = 0
                correct_no = 0
                for sample, sample_label in zip(X_for_weights, y_for_weights):
                    if self._code_matrix[np.where(self._labels == sample_label)[0][0]][clf_idx] != 0:
                        samples_no += 1
                        if clf.predict([sample])[0] == \
                                self._code_matrix[np.where(self._labels == sample_label)[0][0]][clf_idx]:
                            correct_no += 1
                if samples_no != 0:
                    acc = correct_no / samples_no
                    dich_weights[clf_idx] = -1 + 2 * acc
        elif self.weights == &#39;avg_tpr_min&#39;:
            min_counter = Counter([y for y in y_for_weights if y in self.minority_classes])

            for clf_idx, clf in enumerate(self._binary_classifiers):
                min_correct_pred = defaultdict(lambda: 0)
                for sample, sample_label in zip(X_for_weights, y_for_weights):
                    if clf.predict([sample])[0] == \
                            self._code_matrix[np.where(self._labels == sample_label)[0][0]][clf_idx]:
                        min_correct_pred[sample_label] += 1
                avg_tpr_min = np.mean([min_correct_pred[clazz] / min_counter[clazz] for clazz in min_counter.keys()])
                dich_weights[clf_idx] = avg_tpr_min

        self.dich_weights = dich_weights</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="multi_imbalance.ensemble.ecoc.ECOC"><code class="flex name class">
<span>class <span class="ident">ECOC</span></span>
<span>(</span><span>binary_classifier='KNN', preprocessing='SOUP', encoding='OVO', n_neighbors=3, weights=None)</span>
</code></dt>
<dd>
<section class="desc"><p>ECOC (Error Correcting Output Codes) is ensemble method for multi-class classification problems.
Each class is encoded with unique binary or ternary code (where 0 means that class is excluded from training set
of binary classifier). Then in the learning phase each binary classifier is learned. In the decoding phase the class
which is closest to test instance in the sense of Hamming distance is chosen.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>binary_classifier</code></strong> :&ensp;<code>binary</code> <code>classifier</code> <code>used</code> <code>by</code> <code>the</code> <code>algorithm.</code> <code>Possible</code> <code>classifiers</code>:</dt>
<dd>&nbsp;</dd>
</dl>
<ul>
<li>'tree': Decision Tree Classifier,</li>
<li>'NB': Naive Bayes Classifier,</li>
<li>'KNN' : K-Nearest Neighbors</li>
</ul>
<dl>
<dt><strong><code>preprocessing</code></strong> :&ensp;<code>method</code> <code>for</code> <code>oversampling</code> <code>between</code> <code>aggregated</code> <code>classes</code> <code>in</code> <code>each</code> <code>dichotomy.</code> <code>Possible</code> <code>methods</code>:</dt>
<dd>&nbsp;</dd>
</dl>
<ul>
<li>None : no oversampling applied,</li>
<li>'globalCS' : random oversampling - randomly chosen instances of minority classes are duplicated</li>
<li>'SMOTE' : Synthetic Minority Oversampling Technique</li>
<li>'SOUP' : Similarity Oversampling Undersampling Preprocessing</li>
</ul>
<dl>
<dt><strong><code>encoding</code></strong> :&ensp;<code>algorithm</code> <code>for</code> <code>encoding</code> <code>classes.</code> <code>Possible</code> <code>encodings</code>:</dt>
<dd>&nbsp;</dd>
</dl>
<ul>
<li>'dense': ceil(10log2(num_of_classes)) dichotomies, -1 and 1 with probability 0.5 each</li>
<li>'sparse' : ceil(10log2(num_of_classes)) dichotomies, 0 with probability 0.5, -1 and 1 with probability 0.25 each</li>
<li>'OVO' : 'one vs one' - n(n-1)/2 dichotomies, where n is number of classes, one for each pair of classes.
Each column has one 1 and one -1 for classes included in particular pair, 0s for remaining classes.</li>
<li>'OVA' : 'one vs all' - number of dichotomies is equal to number of classes. Each column has one 1 and
-1 for all remaining rows</li>
<li>'complete' : 2^(n-1)-1 dichotomies, reference
T. G. Dietterich and G. Bakiri.
Solving multiclass learning problems via error-correcting output codes.
Journal of Artificial Intelligence Research, 2:263–286, 1995.</li>
</ul>
<dl>
<dt><strong><code>weights</code></strong> :&ensp;<code>strategy</code> <code>for</code> <code>dichotomies</code> <code>weighting.</code> <code>Possible</code> <code>values</code>:</dt>
<dd>&nbsp;</dd>
</dl>
<ul>
<li>None : no weighting applied</li>
<li>'acc' : accuracy-based weights</li>
<li>'avg_tpr_min' : weights based on average true positive rates of dichotomies</li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ECOC:
    &#34;&#34;&#34;
    ECOC (Error Correcting Output Codes) is ensemble method for multi-class classification problems.
    Each class is encoded with unique binary or ternary code (where 0 means that class is excluded from training set
    of binary classifier). Then in the learning phase each binary classifier is learned. In the decoding phase the class
    which is closest to test instance in the sense of Hamming distance is chosen.
    &#34;&#34;&#34;

    _allowed_encodings = [&#39;dense&#39;, &#39;sparse&#39;, &#39;complete&#39;, &#39;OVA&#39;, &#39;OVO&#39;]
    _allowed_oversampling = [None, &#39;globalCS&#39;, &#39;SMOTE&#39;, &#39;SOUP&#39;]
    _allowed_classifiers = [&#39;tree&#39;, &#39;NB&#39;, &#39;KNN&#39;]
    _allowed_weights = [None, &#39;acc&#39;, &#39;avg_tpr_min&#39;]

    def __init__(self, binary_classifier=&#39;KNN&#39;, preprocessing=&#39;SOUP&#39;, encoding=&#39;OVO&#39;, n_neighbors=3,
                 weights=None):
        &#34;&#34;&#34;
        Parameters
        ----------
        binary_classifier: binary classifier used by the algorithm. Possible classifiers:
        * &#39;tree&#39;: Decision Tree Classifier,
        * &#39;NB&#39;: Naive Bayes Classifier,
        * &#39;KNN&#39; : K-Nearest Neighbors

        preprocessing: method for oversampling between aggregated classes in each dichotomy. Possible methods:
        * None : no oversampling applied,
        * &#39;globalCS&#39; : random oversampling - randomly chosen instances of minority classes are duplicated
        * &#39;SMOTE&#39; : Synthetic Minority Oversampling Technique
        * &#39;SOUP&#39; : Similarity Oversampling Undersampling Preprocessing

        encoding : algorithm for encoding classes. Possible encodings:
        * &#39;dense&#39;: ceil(10log2(num_of_classes)) dichotomies, -1 and 1 with probability 0.5 each
        * &#39;sparse&#39; : ceil(10log2(num_of_classes)) dichotomies, 0 with probability 0.5, -1 and 1 with probability 0.25 each
        * &#39;OVO&#39; : &#39;one vs one&#39; - n(n-1)/2 dichotomies, where n is number of classes, one for each pair of classes.
         Each column has one 1 and one -1 for classes included in particular pair, 0s for remaining classes.
        * &#39;OVA&#39; : &#39;one vs all&#39; - number of dichotomies is equal to number of classes. Each column has one 1 and
            -1 for all remaining rows
        * &#39;complete&#39; : 2^(n-1)-1 dichotomies, reference
            T. G. Dietterich and G. Bakiri.
            Solving multiclass learning problems via error-correcting output codes.
            Journal of Artificial Intelligence Research, 2:263–286, 1995.

        weights: strategy for dichotomies weighting. Possible values:
        * None : no weighting applied
        * &#39;acc&#39; : accuracy-based weights
        * &#39;avg_tpr_min&#39; : weights based on average true positive rates of dichotomies

        &#34;&#34;&#34;
        self.binary_classifier = binary_classifier
        self.encoding = encoding
        self.preprocessing = preprocessing
        self.n_neighbors = n_neighbors
        self.weights = weights

        self.minority_classes = list()

        self._code_matrix = None
        self._binary_classifiers = []
        self._labels = None
        self._dich_weights = None

    def fit(self, X, y, minority_classes=None):
        &#34;&#34;&#34;
        Parameters
        ----------
        X: two dimensional numpy array (number of samples x number of features) with float numbers
        y: one dimensional numpy array with labels for rows in X
        minority_classes: list of classes considered to be minority classes
        Returns
        -------
        self: object
        &#34;&#34;&#34;
        if minority_classes is not None:
            self.minority_classes = minority_classes

        if self.weights is not None:
            X_train, X_for_weights, y_train, y_for_weights = train_test_split(X, y, test_size=0.2, stratify=y,
                                                                              random_state=0)
        else:
            X_train, y_train = X, y

        self._labels = np.unique(y)
        self._gen_code_matrix()
        self._binary_classifiers = [self._get_classifier() for _ in range(self._code_matrix.shape[1])]
        self._learn_binary_classifiers(X_train, y_train)
        if self.weights is not None:
            self._calc_weights(X_for_weights, y_for_weights)
        return self

    def predict(self, X):
        &#34;&#34;&#34;
        Parameters
        ----------
        X: two dimensional numpy array (number of samples x number of features) with float numbers
        Returns
        -------
        y : numpy array, shape = [number of samples]
            Predicted target values for X.
        &#34;&#34;&#34;
        output_codes = np.zeros((X.shape[0], self._code_matrix.shape[1]))
        for classifier_idx, classifier in enumerate(self._binary_classifiers):
            output_codes[:, classifier_idx] = classifier.predict(X)

        predicted = np.zeros(X.shape[0])
        for row_idx, encoded_row in enumerate(output_codes):
            predicted[row_idx] = self._get_closest_class(encoded_row)

        return predicted

    def _learn_binary_classifiers(self, X, y):
        for classifier_idx, classifier in enumerate(self._binary_classifiers):
            excluded_classes_indices = [idx for idx in range(len(y)) if
                                        self._code_matrix[self._labels.tolist().index(y[idx])][classifier_idx] == 0]
            X_filtered = np.delete(X, excluded_classes_indices, 0)
            y_filtered = np.delete(y, excluded_classes_indices)
            binary_labels = np.array([self._code_matrix[self._labels.tolist().index(clazz)][classifier_idx] for clazz in
                                      y_filtered])
            X_filtered, binary_labels = self._oversample(X_filtered, binary_labels)
            classifier.fit(X_filtered, binary_labels)

    def _gen_code_matrix(self):
        if self.encoding == &#39;dense&#39;:
            self._code_matrix = self._encode_dense(self._labels.shape[0])
        elif self.encoding == &#39;sparse&#39;:
            self._code_matrix = self._encode_sparse(self._labels.shape[0])
        elif self.encoding == &#39;complete&#39;:
            self._code_matrix = self._encode_complete(self._labels.shape[0])
        elif self.encoding == &#39;OVO&#39;:
            self._code_matrix = self._encode_ovo(self._labels.shape[0])
        elif self.encoding == &#39;OVA&#39;:
            self._code_matrix = self._encode_ova(self._labels.shape[0])
        else:
            raise ValueError(&#34;Unknown matrix generation encoding: %s, expected to be one of %s.&#34;
                             % (self.encoding, ECOC._allowed_encodings))

    def _encode_dense(self, number_of_classes, random_state=0, number_of_code_generations=10000):
        try:
            dirname = os.path.dirname(__file__)
            matrix = np.load(dirname + f&#39;/cached_matrices/dense_{number_of_classes}.npy&#39;)
            return matrix
        except IOError:
            print(f&#39;Could not find cached matrix for dense code for {number_of_classes} classes, generating matrix...&#39;)

        number_of_columns = int(np.ceil(10 * np.log2(number_of_classes)))
        code_matrix = np.ones((number_of_classes, number_of_columns))
        random_state = check_random_state(random_state)

        max_min_dist = 0
        for i in range(number_of_code_generations):
            tmp_code_matrix = np.ones((number_of_classes, number_of_columns))
            min_dist = float(&#39;inf&#39;)

            for row in range(0, number_of_classes):
                for col in range(0, number_of_columns):
                    if random_state.randint(0, 2) == 1:
                        tmp_code_matrix[row, col] = -1

                for compared_row in range(0, row):
                    dist = self._hamming_distance(tmp_code_matrix[compared_row], tmp_code_matrix[row])
                    if dist &lt; min_dist:
                        min_dist = dist

            if min_dist &gt; max_min_dist:
                max_min_dist = min_dist
                code_matrix = tmp_code_matrix
        return code_matrix

    def _encode_sparse(self, number_of_classes, random_state=0, number_of_code_generations=10000):
        try:
            dirname = os.path.dirname(__file__)
            matrix = np.load(dirname + f&#39;/cached_matrices/sparse_{number_of_classes}.npy&#39;)
            return matrix
        except IOError:
            print(f&#39;Could not find cached matrix for sparse code for {number_of_classes} classes, generating matrix...&#39;)

        number_of_columns = int(np.ceil(15 * np.log2(number_of_classes)))
        code_matrix = np.ones((number_of_classes, number_of_columns))
        random_state = check_random_state(random_state)

        max_min_dist = 0
        for i in range(number_of_code_generations):
            tmp_code_matrix = np.ones((number_of_classes, number_of_columns))
            min_dist = float(&#39;inf&#39;)

            for row in range(0, number_of_classes):
                for col in range(0, number_of_columns):
                    rand = random_state.randint(0, 4)
                    if rand &lt; 2:
                        tmp_code_matrix[row, col] = 0
                    elif rand == 3:
                        tmp_code_matrix[row, col] = -1

                if np.count_nonzero(tmp_code_matrix[row]) == 0:
                    break

                for compared_row in range(0, row):
                    dist = self._hamming_distance(tmp_code_matrix[compared_row], tmp_code_matrix[row])
                    if dist &lt; min_dist:
                        min_dist = dist

            if self._has_matrix_all_zeros_column(tmp_code_matrix):
                continue

            if min_dist &gt; max_min_dist:
                max_min_dist = min_dist
                code_matrix = tmp_code_matrix

        return code_matrix

    def _encode_ova(self, number_of_classes):
        matrix = np.identity(number_of_classes)
        matrix[matrix == 0] = -1
        return matrix

    def _encode_ovo(self, number_of_classes):
        number_of_columns = int(number_of_classes * (number_of_classes - 1) / 2)
        matrix = np.zeros((number_of_classes, number_of_columns), dtype=int)
        indices_map = self._map_indices_to_class_pairs(number_of_classes)
        for row in range(number_of_classes):
            for col in range(number_of_columns):
                if indices_map[col][0] == row:
                    matrix[row, col] = 1
                elif indices_map[col][1] == row:
                    matrix[row, col] = -1
        return matrix

    def _map_indices_to_class_pairs(self, number_of_classes):
        indices_map = dict()
        idx = 0
        for i in range(number_of_classes):
            for j in range(i + 1, number_of_classes):
                indices_map[idx] = (i, j)
                idx += 1
        return indices_map

    def _encode_complete(self, number_of_classes):
        code_length = 2 ** (number_of_classes - 1) - 1
        matrix = np.ones((number_of_classes, code_length))
        for row_idx in range(1, number_of_classes):
            digit = -1
            partial_code_len = 2 ** (number_of_classes - row_idx - 1)
            for idx in range(0, code_length, partial_code_len):
                matrix[row_idx][idx:idx + partial_code_len] = digit
                digit *= -1
        return matrix

    def _hamming_distance(self, v1, v2):
        return np.count_nonzero(v1 != v2)

    def _has_matrix_all_zeros_column(self, matrix):
        return (~matrix.any(axis=0)).any()

    def _get_closest_class(self, row):
        if self.weights is not None:
            return self._labels[
                np.argmin(
                    [sum(np.multiply(self.dich_weights, (encoded_class - row) ** 2)) for encoded_class in
                     self._code_matrix])]
        else:
            return self._labels[
                np.argmin([self._hamming_distance(row, encoded_class) for encoded_class in self._code_matrix])]

    def _oversample(self, X, y):
        if self.preprocessing not in ECOC._allowed_oversampling:
            raise ValueError(&#34;Unknown matrix generation encoding: %s, expected to be one of %s.&#34;
                             % (self.encoding, ECOC._allowed_oversampling))
        elif np.unique(y).size == 1:
            return X, y
        elif self.preprocessing is None:
            return X, y
        elif self.preprocessing == &#39;globalCS&#39;:
            gcs = GlobalCS()
            return gcs.fit_transform(X, y)
        elif self.preprocessing == &#39;SMOTE&#39;:
            return self._smote_oversample(X, y)
        elif self.preprocessing == &#39;SOUP&#39;:
            soup = SOUP()
            return soup.fit_transform(X, y)

    def _get_classifier(self):
        if self.binary_classifier not in ECOC._allowed_classifiers:
            raise ValueError(&#34;Unknown binary classifier: %s, expected to be one of %s.&#34;
                             % (self.binary_classifier, ECOC._allowed_classifiers))
        elif self.binary_classifier == &#39;tree&#39;:
            decision_tree_classifier = DecisionTreeClassifier(random_state=42)
            return decision_tree_classifier
        elif self.binary_classifier == &#39;NB&#39;:
            gnb = GaussianNB()
            return gnb
        elif self.binary_classifier == &#39;KNN&#39;:
            knn = KNeighborsClassifier(n_neighbors=self.n_neighbors)
            return knn

    def _smote_oversample(self, X, y):
        n_neighbors = min(3, min(np.unique(y, return_counts=True)[1]) - 1)
        if n_neighbors == 0:
            raise ValueError(
                &#39;In order to use SMOTE preprocessing, the training set should contain at least 2 examples from each class&#39;)
        smote = SMOTE(k_neighbors=n_neighbors, random_state=42)
        return smote.fit_resample(X, y)


    def _calc_weights(self, X_for_weights, y_for_weights):
        if self.weights not in ECOC._allowed_weights:
            raise ValueError(&#34;Unknown weighting strategy: %s, expected to be one of %s.&#34;
                             % (self.weights, ECOC._allowed_weights))

        dich_weights = np.ones(self._code_matrix.shape[1])
        if self.weights == &#39;acc&#39;:
            for clf_idx, clf in enumerate(self._binary_classifiers):
                samples_no = 0
                correct_no = 0
                for sample, sample_label in zip(X_for_weights, y_for_weights):
                    if self._code_matrix[np.where(self._labels == sample_label)[0][0]][clf_idx] != 0:
                        samples_no += 1
                        if clf.predict([sample])[0] == \
                                self._code_matrix[np.where(self._labels == sample_label)[0][0]][clf_idx]:
                            correct_no += 1
                if samples_no != 0:
                    acc = correct_no / samples_no
                    dich_weights[clf_idx] = -1 + 2 * acc
        elif self.weights == &#39;avg_tpr_min&#39;:
            min_counter = Counter([y for y in y_for_weights if y in self.minority_classes])

            for clf_idx, clf in enumerate(self._binary_classifiers):
                min_correct_pred = defaultdict(lambda: 0)
                for sample, sample_label in zip(X_for_weights, y_for_weights):
                    if clf.predict([sample])[0] == \
                            self._code_matrix[np.where(self._labels == sample_label)[0][0]][clf_idx]:
                        min_correct_pred[sample_label] += 1
                avg_tpr_min = np.mean([min_correct_pred[clazz] / min_counter[clazz] for clazz in min_counter.keys()])
                dich_weights[clf_idx] = avg_tpr_min

        self.dich_weights = dich_weights</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="multi_imbalance.ensemble.ecoc.ECOC.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, minority_classes=None)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>two</code> <code>dimensional</code> <code>numpy</code> <code>array</code> (<code>number</code> of <code>samples</code> <code>x</code> <code>number</code> of <code>features</code>) <code>with</code> <code>float</code> <code>numbers</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>one</code> <code>dimensional</code> <code>numpy</code> <code>array</code> <code>with</code> <code>labels</code> <code>for</code> <code>rows</code> <code>in</code> <code>X</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>minority_classes</code></strong> :&ensp;<code>list</code> of <code>classes</code> <code>considered</code> <code>to</code> <code>be</code> <code>minority</code> <code>classes</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y, minority_classes=None):
    &#34;&#34;&#34;
    Parameters
    ----------
    X: two dimensional numpy array (number of samples x number of features) with float numbers
    y: one dimensional numpy array with labels for rows in X
    minority_classes: list of classes considered to be minority classes
    Returns
    -------
    self: object
    &#34;&#34;&#34;
    if minority_classes is not None:
        self.minority_classes = minority_classes

    if self.weights is not None:
        X_train, X_for_weights, y_train, y_for_weights = train_test_split(X, y, test_size=0.2, stratify=y,
                                                                          random_state=0)
    else:
        X_train, y_train = X, y

    self._labels = np.unique(y)
    self._gen_code_matrix()
    self._binary_classifiers = [self._get_classifier() for _ in range(self._code_matrix.shape[1])]
    self._learn_binary_classifiers(X_train, y_train)
    if self.weights is not None:
        self._calc_weights(X_for_weights, y_for_weights)
    return self</code></pre>
</details>
</dd>
<dt id="multi_imbalance.ensemble.ecoc.ECOC.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>two</code> <code>dimensional</code> <code>numpy</code> <code>array</code> (<code>number</code> of <code>samples</code> <code>x</code> <code>number</code> of <code>features</code>) <code>with</code> <code>float</code> <code>numbers</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>y</code></strong> :&ensp;<code>numpy</code> <code>array</code>, <code>shape</code> = [<code>number</code> of <code>samples</code>]</dt>
<dd>Predicted target values for X.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34;
    Parameters
    ----------
    X: two dimensional numpy array (number of samples x number of features) with float numbers
    Returns
    -------
    y : numpy array, shape = [number of samples]
        Predicted target values for X.
    &#34;&#34;&#34;
    output_codes = np.zeros((X.shape[0], self._code_matrix.shape[1]))
    for classifier_idx, classifier in enumerate(self._binary_classifiers):
        output_codes[:, classifier_idx] = classifier.predict(X)

    predicted = np.zeros(X.shape[0])
    for row_idx, encoded_row in enumerate(output_codes):
        predicted[row_idx] = self._get_closest_class(encoded_row)

    return predicted</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="multi_imbalance.ensemble" href="index.html">multi_imbalance.ensemble</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="multi_imbalance.ensemble.ecoc.ECOC" href="#multi_imbalance.ensemble.ecoc.ECOC">ECOC</a></code></h4>
<ul class="">
<li><code><a title="multi_imbalance.ensemble.ecoc.ECOC.fit" href="#multi_imbalance.ensemble.ecoc.ECOC.fit">fit</a></code></li>
<li><code><a title="multi_imbalance.ensemble.ecoc.ECOC.predict" href="#multi_imbalance.ensemble.ecoc.ECOC.predict">predict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>